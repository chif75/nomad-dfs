{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNrzjBd8WAz8R3ZGlxCvu4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setting up initial"],"metadata":{"id":"CSyktGnhs3Ah"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"Fh-19qIZmKwq","executionInfo":{"status":"ok","timestamp":1695650219514,"user_tz":-180,"elapsed":309,"user":{"displayName":"Maya Segal","userId":"14184902003902606354"}}},"outputs":[],"source":["# import pandas as pd\n","# import requests\n","# import os\n","# import re\n","# import time\n","# from time import time\n","# from IPython.display import display, clear_output\n","# from tqdm import tqdm\n","\n","import pandas as pd\n","import requests\n","from bs4 import BeautifulSoup\n","from IPython.display import display, clear_output\n","\n","pd.set_option('display.max_columns', 100)\n","pd.set_option('display.max_rows', 100)\n","pd.set_option('display.max_colwidth', 200)\n","\n"]},{"cell_type":"code","source":["# Read csv file from google drive\n","url='https://drive.google.com/file/d/1RyMAQvNOl8l2KsWSG6yFPXKN3WbHv25N/view?usp=sharing'\n","file_id=url.split('/')[-2]\n","dwn_url='https://drive.google.com/uc?id=' + file_id\n","df_sf_scan = pd.read_csv(dwn_url)\n","\n","print(len(df_sf_scan))\n","\n","df_sf_scan = df_sf_scan[df_sf_scan['Status Code'] == 200]\n","df_crime_in = df_sf_scan['Address'][df_sf_scan['Address'].str.contains('/crime/in/')]\n","\n","print(len(df_sf_scan))\n","print(len(df_crime_in))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C4Q45BRVmP6M","executionInfo":{"status":"ok","timestamp":1695650227298,"user_tz":-180,"elapsed":6463,"user":{"displayName":"Maya Segal","userId":"14184902003902606354"}},"outputId":"d7f4b665-d933-48a3-b468-0b82620ea367"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["183240\n","183204\n","13480\n"]}]},{"cell_type":"code","source":["start_elapsed_time = time()\n","\n","# Function to append data to the CSV files\n","def append_data_to_csv(df, file_name):\n","    with open(file_name, 'a', encoding='utf-8', newline='') as f:\n","        df.to_csv(f, header=f.tell() == 0, index=False)\n","\n","# Initialize an empty list to store the scraped DF's\n","scraped_data_1 = []\n","scraped_data_2 = []\n","start_page = 1  # Starting page number\n","end_page = len(df_crime_in)\n","#end_page = 10   # Ending page number (not inclusive)\n","count = start_page\n","count_status_code_200_yes = 0\n","count_status_code_200_no = 0\n","table_number_1 = 2\n","table_number_2 = 3\n","num_pages_append = 100\n","\n","# Create empty DF's\n","df = pd.DataFrame()\n","df_timing = pd.DataFrame(columns=['url', 'start_time', 'end_time', 'page_count'])\n","\n","# Iterate through the URLs from the desired range and scrape the specific table in the HTML file\n","for index, urls in enumerate(df_crime_in[start_page-1:end_page], start=1):\n","    start_time = time()\n","\n","    response = requests.get(urls)\n","    if response.status_code == 200:\n","        html_content = response.text\n","        read_html_tables = pd.read_html(html_content)\n","        soup = BeautifulSoup(html_content, 'html.parser')\n","\n","        # Check if the table_number is within the valid range\n","        if table_number_1 >= 0 and table_number_1 < len(read_html_tables) and table_number_2 >= 0 and table_number_2 < len(read_html_tables):\n","            df_table_1 = read_html_tables[table_number_1]\n","            df_table_2 = read_html_tables[table_number_2]\n","\n","            ## Concat 2 tables from the html to 1 df_table\n","            #df_table = pd.concat([df_table_1, df_table_2], ignore_index=True)\n","\n","            # Add the 'URL' column containing the current URL value to the DF\n","            df_table_1['URL'] = urls\n","            df_table_2['URL'] = urls\n","\n","            # Extract the country and city from the breadcrumbs element\n","            column_1 = soup.find('span', itemprop='name').text\n","            page_country = soup.find_all('span', itemprop='name')[1].text\n","            page_city = soup.find_all('span', itemprop='name')[2].text\n","\n","            df_table_1['column_1'] = column_1\n","            df_table_1['Country'] = page_country\n","            df_table_1['City'] = page_city\n","            df_table_2['column_1'] = column_1\n","            df_table_2['Country'] = page_country\n","            df_table_2['City'] = page_city\n","\n","            scraped_data_1.append(df_table_1)\n","            scraped_data_2.append(df_table_2)\n","            end_time = time()\n","            clear_output(wait=True)\n","            count += 1\n","            count_status_code_200_yes += 1\n","            display(f'Page {count}/{end_page} | {urls} | {round(end_time - start_time, 2)} sec')\n","\n","            # Add data to df_timing to keep track of numbeo response times\n","            df_timing = pd.concat([df_timing, pd.DataFrame({'url': [urls], 'start_time': [start_time], 'end_time': [end_time], 'page_count': [count]})], ignore_index=True)\n","\n","            # Append data to CSV files every num_pages_append pages or on the last page\n","            if count % num_pages_append == 0 or count == end_page:\n","                if scraped_data_1:\n","                    df_raw_1 = pd.concat(scraped_data_1, ignore_index=True)\n","                    df_1 = df_raw_1\n","\n","                    # Save the DF's to CSV files\n","                    append_data_to_csv(df_1, 'df_output_1.csv')\n","                    append_data_to_csv(df_timing, 'df_timing_output.csv')\n","\n","                    # Clear the scraped_data list and reinitialize df and df_timing\n","                    scraped_data_1 = []\n","                    df_1 = pd.DataFrame()\n","                    df_timing = pd.DataFrame()\n","\n","\n","                if scraped_data_2:\n","                    df_raw_2 = pd.concat(scraped_data_2, ignore_index=True)\n","                    df_2 = df_raw_2\n","\n","                    # Save the DF's to CSV files\n","                    append_data_to_csv(df_2, 'df_output_2.csv')\n","\n","                    # Clear the scraped_data list and reinitialize df and df_timing\n","                    scraped_data_2 = []\n","                    df_2 = pd.DataFrame()\n","\n","                    clear_output(wait=True)\n","                    print('\\033[32mData appended to CSV files!\\033[0m')  # Green color\n","        else:\n","            clear_output(wait=True)\n","            count += 1\n","            count_status_code_200_no += 1\n","            display(f'Page {count}/{end_page-1} | {urls} | Oops... no table')\n","            continue\n","    else:\n","        clear_output(wait=True)\n","        count += 1\n","        count_status_code_200_no += 1\n","        display(f'Page {count}/{end_page-1} | {urls} | Error: Unable to fetch data')\n","        continue\n","\n","# Calculate time_elapsed in seconds\n","time_elapsed = time() - start_elapsed_time\n","# Format time_elapsed as hh:mm:ss\n","formatted_time_elapsed = '{:.0f}:{:.0f}:{:.0f}'.format(time_elapsed // 3600, (time_elapsed % 3600) // 60, time_elapsed % 60)\n","\n","# Final message after scraping all pages\n","clear_output(wait=True)\n","print(f'\\033[32mDone!\\033[0m | Number of scrapped URLs: {count_status_code_200_yes} / {end_page}')  # Green color\n","print(formatted_time_elapsed)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"z7isepRvmuFT","outputId":"60d329fc-c1d6-49b8-f839-02dc63cbbf68"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["'Page 6484/13479 | https://www.numbeo.com/crime/in/Mahebourg-Mauritius | Oops... no table'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"VU0VSUlusn1w"},"execution_count":null,"outputs":[]}]}